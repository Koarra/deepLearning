{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16b9212-2e33-474b-9921-97f18e732934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an RNN (with LSTM) to classify short movie reviews as positive or negative.\n",
    "# Why RNN is suitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be79bdb7-8d4f-41d8-be83-eb2198f006dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I loved the movie!\",        # 1\n",
    "    \"Absolutely terrible.\",      # 0\n",
    "    \"Not bad at all\",            # 1\n",
    "    \"Waste of time.\",            # 0\n",
    "    \"Fantastic performance!\"     # 1\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "170c73bd-7f06-43a9-80c5-346a992695f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1 defaultdict(<function <lambda> at 0x123b13ce0>, {})\n",
      "defaultdict(<function <lambda> at 0x123b13ce0>, {'<PAD>': 0})\n",
      "tokenized [[1, 2, 3, 4], [5, 6], [7, 8, 9, 10], [11, 12, 13], [14, 15]]\n",
      "padded tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  0,  0],\n",
      "        [ 7,  8,  9, 10],\n",
      "        [11, 12, 13,  0],\n",
      "        [14, 15,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "# tokenization and encoding\n",
    "\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Build vocabulary\n",
    "# word2idx: A special dictionary that creates a new unique index for every new word. \n",
    "# At this point, itâ€™s just an empty dictionary with auto-indexing behavior.\n",
    "# for example \"I\" will get a specific ID, and will always have the same\n",
    "word2idx = defaultdict(lambda: len(word2idx)) # defaultdict is a special kind of dictionary, it automatically creates a default value for any key that doesn't exist yet.\n",
    "print(\"step1\", word2idx)\n",
    "\n",
    "# Assign the token <PAD> to index 0. This token will be used to fill empty space.\n",
    "# Padding is important because RNNs require uniform sequence lengths for batching\n",
    "word2idx[\"<PAD>\"] = 0  # padding token\n",
    "tokenized = [[word2idx[word.lower()] for word in text.split()] for text in texts]\n",
    "print(\"tokenized\", tokenized)\n",
    "padded = pad_sequence([torch.tensor(t) for t in tokenized], batch_first=True)\n",
    "print (\"padded\", padded)\n",
    "labels_tensor = torch.tensor(labels).float().unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89f443d-7194-4835-a3c5-1e33b134e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bf75b8-c43e-4fdb-acbf-f86480514029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.7339 | Accuracy: 0.20\n",
      "Epoch 2 | Loss: 0.7079 | Accuracy: 0.40\n",
      "Epoch 3 | Loss: 0.6831 | Accuracy: 0.40\n",
      "Epoch 4 | Loss: 0.6599 | Accuracy: 1.00\n",
      "Epoch 5 | Loss: 0.6384 | Accuracy: 0.80\n",
      "Epoch 6 | Loss: 0.6186 | Accuracy: 0.80\n",
      "Epoch 7 | Loss: 0.6001 | Accuracy: 0.80\n",
      "Epoch 8 | Loss: 0.5820 | Accuracy: 0.80\n",
      "Epoch 9 | Loss: 0.5633 | Accuracy: 0.80\n",
      "Epoch 10 | Loss: 0.5435 | Accuracy: 0.80\n",
      "Epoch 11 | Loss: 0.5222 | Accuracy: 0.80\n",
      "Epoch 12 | Loss: 0.4989 | Accuracy: 0.80\n",
      "Epoch 13 | Loss: 0.4734 | Accuracy: 0.80\n",
      "Epoch 14 | Loss: 0.4456 | Accuracy: 1.00\n",
      "Epoch 15 | Loss: 0.4162 | Accuracy: 1.00\n",
      "Epoch 16 | Loss: 0.3867 | Accuracy: 1.00\n",
      "Epoch 17 | Loss: 0.3586 | Accuracy: 1.00\n",
      "Epoch 18 | Loss: 0.3319 | Accuracy: 1.00\n",
      "Epoch 19 | Loss: 0.3059 | Accuracy: 1.00\n",
      "Epoch 20 | Loss: 0.2805 | Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(len(word2idx))\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(padded)\n",
    "    loss = loss_fn(output, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pred = (output > 0.5).int()\n",
    "    acc = (pred == labels_tensor.int()).float().mean()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f} | Accuracy: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd471d19-ac83-43c6-be43-281a6b1c689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = [\n",
    "    \"I hated the ending.\",\n",
    "    \"What a masterpiece!\",\n",
    "    \"Not bad, could be better.\",\n",
    "    \"This movie was a total waste of time.\",\n",
    "    \"Absolutely loved every second.\", \n",
    "    \"not bad I really like it\"\n",
    "]\n",
    "\n",
    "# Tokenize new samples\n",
    "new_tokenized = [[word2idx[word.lower()] if word.lower() in word2idx else 0\n",
    "                  for word in text.split()] for text in new_texts]\n",
    "\n",
    "# Pad sequences to same length\n",
    "new_padded = pad_sequence([torch.tensor(t) for t in new_tokenized], batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb253e37-c5ab-49f5-8fde-79e7b0013ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hated the ending. â†’ Positive ðŸ˜Š\n",
      "What a masterpiece! â†’ Positive ðŸ˜Š\n",
      "Not bad, could be better. â†’ Negative ðŸ˜ž\n",
      "This movie was a total waste of time. â†’ Negative ðŸ˜ž\n",
      "Absolutely loved every second. â†’ Positive ðŸ˜Š\n",
      "not bad I really like it â†’ Positive ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(new_padded)\n",
    "    predicted_classes = (predictions > 0.5).int().squeeze()\n",
    "\n",
    "for text, pred in zip(new_texts, predicted_classes):\n",
    "    sentiment = \"Positive ðŸ˜Š\" if pred.item() == 1 else \"Negative ðŸ˜ž\"\n",
    "    print(f\"{text} â†’ {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752ba03-115a-4cd7-8fb3-6e66e11caf49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c0f11-63b1-4944-9f0a-58025ff148a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casstle",
   "language": "python",
   "name": "casstle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
